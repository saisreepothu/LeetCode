#!/usr/bin/env python3
"""
Utility script to fetch a LeetCode problem via GraphQL and scaffold local files.
Only metadata, the Readme, and the CSV log are managed here—solution files must
never be generated by this script.

Usage:
    python scripts/create_problem.py <leetcode-problem-url>
"""

from __future__ import annotations

import argparse
import csv
import datetime as _dt
import json
import pathlib
import re
import sys
from requests.exceptions import RequestException
from typing import Any, Dict, List

import requests


GRAPHQL_ENDPOINT = "https://leetcode.com/graphql"
BASE_URL = "https://leetcode.com"
REQUEST_TIMEOUT_SECONDS = 20
USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36"
)

GRAPHQL_QUERY = """
query questionData($titleSlug: String!) {
  question(titleSlug: $titleSlug) {
    questionId
    questionFrontendId
    title
    titleSlug
    content
    difficulty
    topicTags {
      name
      slug
    }
  }
}
""".strip()


def parse_slug(url: str) -> str:
    match = re.search(r"leetcode\.com/problems/([^/]+)/?", url)
    if not match:
        raise ValueError(f"Could not parse problem slug from URL: {url}")
    slug = match.group(1).strip()
    if not slug:
        raise ValueError("Problem slug is empty after parsing URL.")
    return slug


def fetch_problem_data(slug: str) -> Dict[str, Any]:
    session = requests.Session()
    session.headers.update(
        {
            "User-Agent": USER_AGENT,
            "Referer": f"{BASE_URL}/problems/{slug}/",
            "Origin": BASE_URL,
            "Content-Type": "application/json",
        }
    )

    payload = {"query": GRAPHQL_QUERY, "variables": {"titleSlug": slug}}
    try:
        resp = session.post(
            GRAPHQL_ENDPOINT, json=payload, timeout=REQUEST_TIMEOUT_SECONDS
        )
        resp.raise_for_status()
    except RequestException as exc:
        raise RuntimeError(f"Network/API request failed: {exc}") from exc

    data = resp.json()
    if "errors" in data and data["errors"]:
        msg = data["errors"][0].get("message", "Unknown GraphQL error")
        raise RuntimeError(f"GraphQL returned an error: {msg}")

    question = data.get("data", {}).get("question")
    if not question:
        raise RuntimeError("No question data returned from GraphQL API.")
    return question


def build_metadata(problem: Dict[str, Any], url: str) -> Dict[str, Any]:
    today = _dt.date.today()
    reviews = {
        "1d": (today + _dt.timedelta(days=1)).isoformat(),
        "7d": (today + _dt.timedelta(days=7)).isoformat(),
        "30d": (today + _dt.timedelta(days=30)).isoformat(),
    }
    return {
        "id": int(problem["questionFrontendId"]),
        "slug": problem["titleSlug"],
        "difficulty": problem["difficulty"].lower(),
        "topics": [tag["slug"] for tag in problem.get("topicTags", [])],
        "date_solved": today.isoformat(),
        "url": url,
        "attempts": 1,
        "reviews": reviews,
    }


def normalize_problem_content(content: str) -> str:
    """Light cleanup so LeetCode HTML renders better in local Markdown previews."""
    cleaned = content.strip()
    cleaned = re.sub(r"<div[^>]*>", "", cleaned)
    cleaned = cleaned.replace("</div>", "")
    cleaned = re.sub(r"<pre[^>]*>", "<pre>", cleaned)

    lines = []
    prev_line = None
    for raw_line in cleaned.splitlines():
        line = raw_line.rstrip()
        if line and line == prev_line:
            continue
        lines.append(line)
        prev_line = line

    return "\n".join(lines).strip()


def write_files(problem: Dict[str, Any], metadata: Dict[str, Any]) -> None:
    """Persist the Readme/metadata only—solution stub files are created later by the user."""
    slug = problem["titleSlug"]
    problem_id = metadata["id"]
    problem_dir = pathlib.Path("problems") / f"{problem_id}_{slug}"
    problem_dir.mkdir(parents=True, exist_ok=True)

    readme = problem_dir / "Readme.md"
    normalized_content = normalize_problem_content(problem["content"])
    readme_content = (
        f"# {problem['title']}\n\nSource: {metadata['url']}\n\n{normalized_content}\n"
    )
    readme.write_text(readme_content)

    metadata_path = problem_dir / "metadata.json"
    metadata_path.write_text(json.dumps(metadata, indent=2))

    append_problem_csv(metadata)

    print(f"Created {readme}")
    print(f"Created {metadata_path}")


def append_problem_csv(metadata: Dict[str, Any]) -> None:
    csv_path = pathlib.Path("problems.csv")
    header = [
        "id",
        "slug",
        "difficulty",
        "topics",
        "date_solved",
        "url",
        "attempts",
        "reviews_1d",
        "reviews_7d",
        "reviews_30d",
    ]
    reviews = metadata.get("reviews", {})
    new_row = {
        "id": str(metadata.get("id", "")),
        "slug": str(metadata.get("slug", "")),
        "difficulty": str(metadata.get("difficulty", "")),
        "topics": ";".join(metadata.get("topics", [])),
        "date_solved": str(metadata.get("date_solved", "")),
        "url": str(metadata.get("url", "")),
        "attempts": str(metadata.get("attempts", "")),
        "reviews_1d": str(reviews.get("1d", "")),
        "reviews_7d": str(reviews.get("7d", "")),
        "reviews_30d": str(reviews.get("30d", "")),
    }

    rows: List[Dict[str, str]] = []
    if csv_path.exists() and csv_path.stat().st_size > 0:
        with csv_path.open(newline="") as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                normalized = {key: row.get(key, "") for key in header}
                rows.append(normalized)

    replaced = False
    for idx, row in enumerate(rows):
        if row.get("id") == new_row["id"] and row.get("slug") == new_row["slug"]:
            rows[idx] = new_row
            replaced = True
            break
    if not replaced:
        rows.append(new_row)

    rows.sort(key=lambda r: int(r["id"]) if str(r.get("id", "")).isdigit() else 10**9)

    with csv_path.open("w", newline="") as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames=header)
        writer.writeheader()
        writer.writerows(rows)


def main(argv: list[str]) -> int:
    parser = argparse.ArgumentParser(
        description="Create local scaffolding for a LeetCode problem."
    )
    parser.add_argument("url", help="Full LeetCode problem URL")
    args = parser.parse_args(argv)

    url = args.url.strip()
    print("Starting fetch...")
    slug = parse_slug(url)
    print(f"Parsed slug: {slug}")
    print("Fetching problem data via GraphQL...")
    problem = fetch_problem_data(slug)
    print("Building metadata...")
    metadata = build_metadata(problem, url)
    print("Writing files...")
    write_files(problem, metadata)
    print("All done.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
