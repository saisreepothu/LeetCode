#!/usr/bin/env python3
"""
Utility script to fetch a LeetCode problem via GraphQL and scaffold local files.
Only metadata, the Readme, and the CSV log are managed here—solution files must
never be generated by this script.

Usage:
    python scripts/create_problem.py <leetcode-problem-url>
"""

from __future__ import annotations

import argparse
import csv
import datetime as _dt
import json
import pathlib
import re
import sys
from requests.exceptions import RequestException
from typing import Any, Dict

import requests


GRAPHQL_ENDPOINT = "https://leetcode.com/graphql"
BASE_URL = "https://leetcode.com"
REQUEST_TIMEOUT_SECONDS = 20
USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36"
)

GRAPHQL_QUERY = """
query questionData($titleSlug: String!) {
  question(titleSlug: $titleSlug) {
    questionId
    questionFrontendId
    title
    titleSlug
    content
    difficulty
    topicTags {
      name
      slug
    }
  }
}
""".strip()


def parse_slug(url: str) -> str:
    match = re.search(r"leetcode\.com/problems/([^/]+)/?", url)
    if not match:
        raise ValueError(f"Could not parse problem slug from URL: {url}")
    slug = match.group(1).strip()
    if not slug:
        raise ValueError("Problem slug is empty after parsing URL.")
    return slug


def fetch_problem_data(slug: str) -> Dict[str, Any]:
    session = requests.Session()
    session.headers.update(
        {
            "User-Agent": USER_AGENT,
            "Referer": f"{BASE_URL}/problems/{slug}/",
            "Origin": BASE_URL,
            "Content-Type": "application/json",
        }
    )

    payload = {"query": GRAPHQL_QUERY, "variables": {"titleSlug": slug}}
    try:
        resp = session.post(
            GRAPHQL_ENDPOINT, json=payload, timeout=REQUEST_TIMEOUT_SECONDS
        )
        resp.raise_for_status()
    except RequestException as exc:
        raise RuntimeError(f"Network/API request failed: {exc}") from exc

    data = resp.json()
    if "errors" in data and data["errors"]:
        msg = data["errors"][0].get("message", "Unknown GraphQL error")
        raise RuntimeError(f"GraphQL returned an error: {msg}")

    question = data.get("data", {}).get("question")
    if not question:
        raise RuntimeError("No question data returned from GraphQL API.")
    return question


def build_metadata(problem: Dict[str, Any], url: str) -> Dict[str, Any]:
    today = _dt.date.today()
    reviews = {
        "1d": (today + _dt.timedelta(days=1)).isoformat(),
        "7d": (today + _dt.timedelta(days=7)).isoformat(),
        "30d": (today + _dt.timedelta(days=30)).isoformat(),
    }
    return {
        "id": int(problem["questionFrontendId"]),
        "slug": problem["titleSlug"],
        "difficulty": problem["difficulty"].lower(),
        "topics": [tag["slug"] for tag in problem.get("topicTags", [])],
        "date_solved": today.isoformat(),
        "url": url,
        "attempts": 1,
        "reviews": reviews,
    }


def write_files(problem: Dict[str, Any], metadata: Dict[str, Any]) -> None:
    """Persist the Readme/metadata only—solution stub files are created later by the user."""
    slug = problem["titleSlug"]
    problem_id = metadata["id"]
    problem_dir = pathlib.Path("problems") / f"{problem_id}_{slug}"
    problem_dir.mkdir(parents=True, exist_ok=True)

    readme = problem_dir / "Readme.md"
    readme_content = f"# {problem['title']}\n\nSource: {metadata['url']}\n\n{problem['content'].strip()}\n"
    readme.write_text(readme_content)

    metadata_path = problem_dir / "metadata.json"
    metadata_path.write_text(json.dumps(metadata, indent=2))

    append_problem_csv(metadata)

    print(f"Created {readme}")
    print(f"Created {metadata_path}")


def append_problem_csv(metadata: Dict[str, Any]) -> None:
    csv_path = pathlib.Path("problems.csv")
    header = [
        "id",
        "slug",
        "difficulty",
        "topics",
        "date_solved",
        "url",
        "attempts",
        "reviews_1d",
        "reviews_7d",
        "reviews_30d",
    ]
    if not csv_path.exists() or csv_path.stat().st_size == 0:
        csv_path.write_text(",".join(header) + "\n")

    reviews = metadata.get("reviews", {})
    row = [
        metadata.get("id", ""),
        metadata.get("slug", ""),
        metadata.get("difficulty", ""),
        ";".join(metadata.get("topics", [])),
        metadata.get("date_solved", ""),
        metadata.get("url", ""),
        metadata.get("attempts", ""),
        reviews.get("1d", ""),
        reviews.get("7d", ""),
        reviews.get("30d", ""),
    ]

    with csv_path.open("a", newline="") as csv_file:
        writer = csv.writer(csv_file)
        writer.writerow(row)


def main(argv: list[str]) -> int:
    parser = argparse.ArgumentParser(
        description="Create local scaffolding for a LeetCode problem."
    )
    parser.add_argument("url", help="Full LeetCode problem URL")
    args = parser.parse_args(argv)

    url = args.url.strip()
    print("Starting fetch...")
    slug = parse_slug(url)
    print(f"Parsed slug: {slug}")
    print("Fetching problem data via GraphQL...")
    problem = fetch_problem_data(slug)
    print("Building metadata...")
    metadata = build_metadata(problem, url)
    print("Writing files...")
    write_files(problem, metadata)
    print("All done.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
